#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dump_sizes_qwen_30layers.py

输出用于 HPP 规划的 weights_size / activation_size，长度固定为 30：
- [0..27]  -> 28 个 Transformer block
- [28]     -> norm
- [29]     -> lm_head

Stage0 = [0,14), Stage1 = [14,30)，pipeline 边界层 = 13。
"""

import argparse
from typing import List
import sys, json
import torch
from transformers import AutoConfig, AutoModelForCausalLM

DTYPE_BYTES = {"fp32": 4, "bf16": 2, "fp16": 2}

def bytes_h(n: int) -> str:
    return f"{n/1024/1024:.2f} MiB"

def count_params_bytes(module: torch.nn.Module, bytes_per_elem: int, only_trainable=True) -> int:
    total = 0
    for p in module.parameters(recurse=True):
        if only_trainable and not p.requires_grad:
            continue
        total += p.numel() * bytes_per_elem
    return total

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model", default="Qwen/Qwen3-0.6B-Base")
    ap.add_argument("--seq-len", type=int, default=128, help="用于激活大小的 T")
    ap.add_argument("--comm-dtype", choices=list(DTYPE_BYTES.keys()), default="fp32",
                    help="梯度 AllReduce 的通信 dtype（决定 weights_size 的字节/元素）")
    ap.add_argument("--act-dtype", choices=list(DTYPE_BYTES.keys()), default="fp32",
                    help="激活按此 dtype 计字节")
    ap.add_argument("--include-lm-head-acts", action="store_true",
                    help="计算 lm_head 的 logits 激活大小 (T*V)；默认不计（设为 None）")
    ap.add_argument("--out-py", type=str, default=None,
                    help="将结果写入指定 .py 文件（包含 weights_size/activation_size 两个列表）")
    ap.add_argument("--trust-remote-code", action="store_true", default=True)
    args = ap.parse_args()

    cfg = AutoConfig.from_pretrained(args.model, trust_remote_code=args.trust_remote_code)
    model = AutoModelForCausalLM.from_pretrained(args.model, trust_remote_code=args.trust_remote_code)

    # 28 个 Transformer 层
    blocks: List[torch.nn.Module] = list(model.model.layers)
    assert len(blocks) == 28, f"期望 28 个 Transformer 层，实际检测到 {len(blocks)}"

    H = int(getattr(cfg, "hidden_size"))
    V = int(getattr(cfg, "vocab_size"))
    T = int(args.seq_len)

    comm_b = DTYPE_BYTES[args.comm_dtype]
    act_b  = DTYPE_BYTES[args.act_dtype]

    # ---- weights_size ----
    weights_size: List[int] = []
    for i, blk in enumerate(blocks):
        b = count_params_bytes(blk, comm_b, only_trainable=True)
        weights_size.append(b)

    # 附加层：norm + lm_head
    assert hasattr(model.model, "norm") and model.model.norm is not None
    assert hasattr(model, "lm_head") and model.lm_head is not None

    norm_bytes   = count_params_bytes(model.model.norm, comm_b, only_trainable=True)
    lm_head_bytes= count_params_bytes(model.lm_head,      comm_b, only_trainable=True)

    weights_size.append(norm_bytes)     # idx 28
    weights_size.append(lm_head_bytes)  # idx 29

    # ---- activation_size（单样本）----
    per_sample_block = T * H * act_b
    activation_size: List[object] = [per_sample_block for _ in range(28)]

    # norm 输出形状与 block 一致 (B,T,H)
    activation_size.append(per_sample_block)  # idx 28
    # lm_head：默认不计 logits (B,T,V)，避免过大；如需计入可打开开关
    if args.include_lm_head_acts:
        activation_size.append(T * V * act_b)  # idx 29
    else:
        activation_size.append(None)           # idx 29

    # ---- 打印结果 ----
    print("num_layers = 30")
    print("stage0 = [0, 14)    # 层 0–13")
    print("stage1 = [14, 30)   # 层 14–27 + norm + lm_head")
    print("boundary_layer = 13\n")

    print("# weights_size (bytes) 长度=30：")
    print("weights_size = [")
    for i, b in enumerate(weights_size):
        tag = ""
        if i == 28: tag = "  # norm"
        elif i == 29: tag = "  # lm_head"
        print(f"  {b},{tag}")
    print("]\n")

    print("# activation_size (bytes per *sample*) 长度=30：")
    print("activation_size = [")
    for i, a in enumerate(activation_size):
        tag = ""
        if i == 28: tag = "  # norm"
        elif i == 29: tag = "  # lm_head (logits)" if args.include_lm_head_acts else "  # lm_head (None)"
        print(f"  {repr(a)},{tag}")
    print("]\n")

    # 汇总
    sum_w = sum(b for b in weights_size if isinstance(b, int))
    print("Summary:")
    print(f"  comm dtype: {args.comm_dtype} ({comm_b} B/elem), act dtype: {args.act_dtype} ({act_b} B/elem)")
    print(f"  seq_len (T): {T}, hidden_size (H): {H}, vocab_size (V): {V}")
    print(f"  total trainable param bytes counted: {sum_w} ({bytes_h(sum_w)})")

    # 导出 .py 便于直接 import
    if args.out_py:
        with open(args.out_py, "w", encoding="utf-8") as f:
            f.write("# Auto-generated by dump_sizes_qwen_30layers.py\n")
            f.write("num_layers = 30\n")
            f.write("stage0 = (0, 14)\n")
            f.write("stage1 = (14, 30)\n")
            f.write("boundary_layer = 13\n\n")
            f.write("weights_size = [\n")
            for b in weights_size:
                f.write(f"    {b},\n")
            f.write("]\n\n")
            f.write("activation_size = [\n")
            for a in activation_size:
                f.write(f"    {repr(a)},\n")
            f.write("]\n")
        print(f"\n已写出到 {args.out_py}")

if __name__ == "__main__":
    main()
